{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import collections\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import d4rl\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pyrootutils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = pyrootutils.find_root(search_from=os.path.abspath(''), indicator=\".project-root\")\n",
    "pyrootutils.set_root(path = path,\n",
    "                     project_root_env_var = True,\n",
    "                     dotenv = True,\n",
    "                     pythonpath = True)\n",
    "                     \n",
    "PATH = str(path).replace(\"\\\\\",\"/\")\n",
    "\n",
    "from mlp.mlp_model import MLP_ED\n",
    "from transformer.gpt_transformer.src.utils import (D4RLTrajectoryDataset, make_dir, check_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "\n",
    "    # hyperparameter\n",
    "    env_name = 'halfcheetah'\n",
    "    dataset ='medium'\n",
    "        \n",
    "    batch_size = 128\n",
    "    embed_dim = 1024\n",
    "    k = 31\n",
    "    n_layers = 2\n",
    "        \n",
    "    total_updates = 0\n",
    "    min_total_log_loss = 1e10\n",
    "\n",
    "    wt_decay = 0.005\n",
    "    lr = 0.0002\n",
    "    warmup_steps= 1000\n",
    "    training_steps= 60000\n",
    "    num_updates_per_iter= 100\n",
    "    max_train_iters = training_steps // num_updates_per_iter\n",
    "        \n",
    "    state_weight= 1\n",
    "    reward_weight= 1\n",
    "\n",
    "    wandb_project=\"train-MLP\"\n",
    "    wandb_entity=\"\"\n",
    "    wandb_group=\"\"\n",
    "\n",
    "    # evaluation parameter\n",
    "    # max_eval_ep_len = 1000      # max len of one evaluation episode\n",
    "    # num_eval_ep = 10            # num of evaluation episodes per iteration\n",
    "    \n",
    "    # wandb\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=wandb_project,\n",
    "        entity=wandb_entity,\n",
    "        group=wandb_group,\n",
    "        name=f\"env-{env_name}-data-{dataset}-batch-{batch_size}-iter-{max_train_iters}\"\n",
    "    )    \n",
    "    \n",
    "    if env_name == 'hopper':\n",
    "        env = gym.make('Hopper-v2')\n",
    "\n",
    "    elif env_name == 'halfcheetah':\n",
    "        env = gym.make('HalfCheetah-v2')\n",
    "\n",
    "    elif env_name == 'walker2d':\n",
    "        env = gym.make('Walker2d-v2')\n",
    "\n",
    "    \n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    \n",
    "    TRAIN_DATA_PATH = f'transformer/gpt_transformer/src/data/train/{env_name}-{dataset}-v2.pkl'\n",
    "    VAL_DATA_PATH = f'transformer/gpt_transformer/src/data/val/{env_name}-{dataset}-v2.pkl'\n",
    "    # ORIGINAL_DATA_PATH = f'transformer/gpt_transformer/src/data/original/{args.env_name}-{args.dataset}-v2.pkl'\n",
    "    LOG_PATH = \"mlp/log/\"\n",
    "    make_dir(LOG_PATH)\n",
    "    BEST_MODEL_PATH = \"mlp/best_model/\"\n",
    "    make_dir(BEST_MODEL_PATH)\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = torch.device('cuda:0')\n",
    "    else:\n",
    "        DEVICE = torch.device('cpu')\n",
    "        \n",
    "\n",
    "    # load validate preprocessing(normalization, fit padding) data\n",
    "\n",
    "    val_traj_dataset = D4RLTrajectoryDataset(TRAIN_DATA_PATH, k, val=True, val_dataset_path=VAL_DATA_PATH)\n",
    "    batch_size = check_batch(batch_size, len(val_traj_dataset))\n",
    "\n",
    "    # load train preprocessing(normalization, fit padding) data\n",
    "\n",
    "    train_traj_dataset = D4RLTrajectoryDataset(TRAIN_DATA_PATH, k)\n",
    "    train_traj_data_loader = DataLoader(train_traj_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            drop_last=True)\n",
    "                            \n",
    "    train_data_iter = iter(train_traj_data_loader)\n",
    "\n",
    "    # define model\n",
    "    model = MLP_ED(\n",
    "                state_dim=state_dim,\n",
    "                action_dim=act_dim,\n",
    "                n_embd=embed_dim,\n",
    "                n_layers=n_layers,\n",
    "            ).to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "                        model.parameters(), \n",
    "                        lr=lr, \n",
    "                        weight_decay=wt_decay\n",
    "                    )\n",
    "\n",
    "\n",
    "    def _schedule(step):\n",
    "\n",
    "        # warmp for 1000 steps\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "\n",
    "        # then cosine decay\n",
    "        step = step - warmup_steps\n",
    "        return 0.5 * (\n",
    "            1 + np.cos(step / (training_steps - warmup_steps) * np.pi)\n",
    "        )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda = _schedule\n",
    "        )\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    start_time = datetime.now().replace(microsecond=0)\n",
    "\n",
    "    start_time_str = start_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "    \n",
    "    prefix = f\"{env_name}-{dataset}\"\n",
    "\n",
    "    save_model_name =  f'{prefix}_model.pt'\n",
    "    save_best_model_name = f'{prefix}_model_best.pt'\n",
    "    save_model_path = os.path.join(LOG_PATH, save_model_name)\n",
    "    save_best_model_path = os.path.join(BEST_MODEL_PATH, save_best_model_name)\n",
    "\n",
    "    log_csv_name = prefix + \"_log_\" + start_time_str + \".csv\"\n",
    "    log_csv_path = os.path.join(LOG_PATH, log_csv_name)\n",
    "\n",
    "\n",
    "    csv_writer = csv.writer(open(log_csv_path, 'a', 1))\n",
    "    csv_header = ([\"duration\", \"num_updates\", \"total_loss\", \"state_loss\", \"reward_loss\", \"val_total_loss\", \"val_state_loss\", \"val_reward_loss\"])\n",
    "\n",
    "    csv_writer.writerow(csv_header)\n",
    "\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"start time: \" + start_time_str)\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"device set to: \" + str(DEVICE))\n",
    "    print(\"dataset: \" + prefix)\n",
    "    print(\"batch_size: \" + str(batch_size))\n",
    "    print(\"best model save path: \" + save_best_model_path)\n",
    "    print(\"log csv save path: \" + log_csv_path)\n",
    "\n",
    "    # train\n",
    "    for i_train_iter in tqdm(range(max_train_iters)):\n",
    "\n",
    "\n",
    "        log_state_losses, log_reward_losses, log_total_losses = [], [], []\n",
    "        val_log_state_losses, val_log_reward_losses, val_log_total_losses = [], [], []\n",
    "        model.train()\n",
    "        \n",
    "        for _ in range(num_updates_per_iter):\n",
    "            try:\n",
    "                timesteps, states, next_states, actions, rewards, traj_mask, terminals = next(train_data_iter)\n",
    "            except StopIteration:\n",
    "                train_traj_data_loader = DataLoader(train_traj_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        pin_memory=True,\n",
    "                                        drop_last=True)\n",
    "                                        \n",
    "                train_data_iter = iter(train_traj_data_loader)\n",
    "                timesteps, states, next_states, actions, rewards, traj_mask, terminals = next(train_data_iter)\n",
    "\n",
    "            timesteps = timesteps.to(DEVICE)\t# B x T\n",
    "            states = states.to(DEVICE)\t\t\t# B x T x state_dim\n",
    "            next_states = next_states.to(DEVICE) # B X T X state_dim\n",
    "            actions = actions.to(DEVICE)\t\t# B x T x act_dim\n",
    "            rewards = rewards.to(DEVICE).unsqueeze(dim=-1) # B x T x 1\n",
    "            traj_mask = traj_mask.to(DEVICE)\t# B x T\n",
    "\n",
    "            next_states_target = torch.clone(next_states).detach().to(DEVICE)\n",
    "            rewards_target = torch.clone(rewards).detach().to(DEVICE)\n",
    "        \n",
    "            next_state_preds, rewards_preds = model.forward(\n",
    "                                                            states=states,\n",
    "                                                            actions=actions,\n",
    "                                                        )\n",
    "\n",
    "            # only consider non padded elements\n",
    "            next_state_preds = next_state_preds.view(-1, state_dim)[traj_mask.view(-1,) > 0]\n",
    "            next_states_target = next_states_target.view(-1, state_dim)[traj_mask.view(-1,) > 0]\n",
    "            \n",
    "            rewards_preds = rewards_preds.view(-1, 1)[traj_mask.view(-1,) > 0]\n",
    "            rewards_target = rewards_target.view(-1, 1)[traj_mask.view(-1,) > 0]\n",
    "\n",
    "            state_loss = F.mse_loss(next_state_preds, next_states_target, reduction='mean') * state_weight\n",
    "            reward_loss = F.mse_loss(rewards_preds, rewards_target, reduction='mean') * reward_weight\n",
    "            \n",
    "            total_loss = state_loss.add(reward_loss)\n",
    "            total_loss = torch.mean(total_loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            \n",
    "            #save loss\n",
    "            log_state_losses.append(state_loss.detach().cpu().item())\n",
    "            log_reward_losses.append(reward_loss.detach().cpu().item())\n",
    "            log_total_losses.append(total_loss.detach().cpu().item())\n",
    "            \n",
    "        \n",
    "            \n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_traj_data_loader = DataLoader(val_traj_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            drop_last=True)\n",
    "        for val_timesteps, val_states, val_next_states, val_actions, val_rewards, val_traj_mask, val_terminals in val_traj_data_loader:\n",
    "            \n",
    "            val_timesteps = val_timesteps.to(DEVICE)\t# B x T\n",
    "            val_states = val_states.to(DEVICE)\t\t\t# B x T x state_dim\n",
    "            val_next_states = val_next_states.to(DEVICE) # B X T X state_dim\n",
    "            val_actions = val_actions.to(DEVICE)\t\t# B x T x act_dim\n",
    "            val_rewards = val_rewards.to(DEVICE).unsqueeze(dim=-1) # B x T x 1\n",
    "            val_traj_mask = val_traj_mask.to(DEVICE)\t# B x T\n",
    "                    \n",
    "            val_next_states_target = torch.clone(val_next_states).detach().to(DEVICE)\n",
    "            val_rewards_target = torch.clone(val_rewards).detach().to(DEVICE)\n",
    "            \n",
    "            val_next_state_preds, val_rewards_preds = model.forward(\n",
    "                                                            states=val_states,\n",
    "                                                            actions=val_actions,\n",
    "                                                        )\n",
    "                                                        \n",
    "            # only consider non padded elements\n",
    "            val_next_state_preds = val_next_state_preds.view(-1, state_dim)[val_traj_mask.view(-1,) > 0]\n",
    "            val_next_states_target = val_next_states_target.view(-1, state_dim)[val_traj_mask.view(-1,) > 0]\n",
    "            \n",
    "            val_rewards_preds = val_rewards_preds.view(-1, 1)[val_traj_mask.view(-1,) > 0]\n",
    "            val_rewards_target = val_rewards_target.view(-1, 1)[val_traj_mask.view(-1,) > 0]\n",
    "\n",
    "            val_state_loss = F.mse_loss(val_next_state_preds, val_next_states_target, reduction='mean') * state_weight\n",
    "            val_reward_loss = F.mse_loss(val_rewards_preds, val_rewards_target, reduction='mean') * reward_weight\n",
    "\n",
    "            # todo: try to use mae\n",
    "            \n",
    "            val_total_loss = val_state_loss.add(val_reward_loss)\n",
    "            val_total_loss = torch.mean(val_total_loss)\n",
    "            \n",
    "            # save val loss\n",
    "            val_log_state_losses.append(val_state_loss.detach().cpu().item())\n",
    "            val_log_reward_losses.append(val_reward_loss.detach().cpu().item())\n",
    "            val_log_total_losses.append(val_total_loss.detach().cpu().item())\n",
    "            \n",
    "            \n",
    "        \n",
    "        mean_total_log_loss = np.mean(log_total_losses)\n",
    "        mean_state_log_loss = np.mean(log_state_losses)\n",
    "        mean_reward_log_loss = np.mean(log_reward_losses)\n",
    "        \n",
    "        mean_val_total_log_loss = np.mean(val_log_total_losses)\n",
    "        mean_val_state_log_loss = np.mean(val_log_state_losses)\n",
    "        mean_val_reward_log_loss = np.mean(val_log_reward_losses)\n",
    "\n",
    "\n",
    "        time_elapsed = str(datetime.now().replace(microsecond=0) - start_time)\n",
    "\n",
    "        total_updates += num_updates_per_iter\n",
    "\n",
    "        log_str = (\"=\" * 60 + '\\n' +\n",
    "                \"time elapsed: \" + time_elapsed  + '\\n' +\n",
    "                \"num of updates: \" + str(total_updates) + '\\n' +\n",
    "                \"train total loss: \" + format(mean_total_log_loss, \".5f\") + '\\n' +\n",
    "                \"train state loss: \" + format(mean_state_log_loss, \".5f\") + '\\n' +\n",
    "                \"train reward loss: \" +  format(mean_reward_log_loss, \".5f\") + '\\n' +\n",
    "                \"val total loss: \" + format(mean_val_total_log_loss, \".5f\") + '\\n' +\n",
    "                \"val state loss: \" + format(mean_val_state_log_loss, \".5f\") + '\\n' +\n",
    "                \"val reward loss: \" +  format(mean_val_reward_log_loss, \".5f\")\n",
    "                )\n",
    "\n",
    "        print(log_str)\n",
    "\n",
    "        log_data = [time_elapsed, total_updates, mean_total_log_loss, mean_state_log_loss, mean_reward_log_loss,\n",
    "            mean_val_total_log_loss, mean_val_state_log_loss, mean_val_reward_log_loss]\n",
    "\n",
    "        csv_writer.writerow(log_data)\n",
    "        \n",
    "        # save model\n",
    "        if mean_val_total_log_loss <= min_total_log_loss:\n",
    "            print(\"saving min loss model at: \" + save_best_model_path)\n",
    "            torch.save(model.state_dict(), save_best_model_path)\n",
    "            min_total_log_loss = mean_val_total_log_loss\n",
    "\n",
    "        print(\"saving current model at: \" + save_model_path)\n",
    "        torch.save(model.state_dict(), save_model_path)\n",
    "\n",
    "        # wandb log\n",
    "        wandb.log({'Iteration': i_train_iter, 'train_loss': mean_total_log_loss, 'validation_loss': mean_val_total_log_loss})\n",
    "\n",
    "    # finish wandb\n",
    "    wandb.finish()\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"finished training!\")\n",
    "    print(\"=\" * 60)\n",
    "    end_time = datetime.now().replace(microsecond=0)\n",
    "    time_elapsed = str(end_time - start_time)\n",
    "    end_time_str = end_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "    print(\"started training at: \" + start_time_str)\n",
    "    print(\"finished training at: \" + end_time_str)\n",
    "    print(\"total training time: \" + time_elapsed)\n",
    "    print(\"saved min loss model at: \" + save_best_model_path)\n",
    "    print(\"saved last updated model at: \" + save_model_path)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c17fe1797bafa14403cfda96b26bd1af359b6a580559a5a3e23455173caa4feb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
