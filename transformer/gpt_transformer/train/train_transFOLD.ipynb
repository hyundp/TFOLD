{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import collections\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import d4rl\n",
    "import gym\n",
    "import numpy as np\n",
    "import pyrootutils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = pyrootutils.find_root(search_from=os.path.abspath(''), indicator=\".project-root\")\n",
    "pyrootutils.set_root(path = path,\n",
    "                     project_root_env_var = True,\n",
    "                     dotenv = True,\n",
    "                     pythonpath = True)\n",
    "                     \n",
    "PATH = str(path).replace(\"\\\\\",\"/\")\n",
    "\n",
    "from transformer.gpt_transformer.src.model import DecisionTransformer\n",
    "from transformer.gpt_transformer.src.utils import (D4RLTrajectoryDataset,\n",
    "                                                   check_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mujoco-py check passed\n",
      "d4rl check passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n",
      "pybullet build time: Apr 30 2024 12:01:25\n",
      "c:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\gym\\spaces\\box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "# test mujoco, d4rl\n",
    "\n",
    "!python ./check/mujoco_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N:  1000000\n",
      "env name:  halfcheetah-medium-v2\n",
      "original n of traj:  1000\n",
      "train n of traj:  800\n",
      "val n of traj:  200\n",
      "-----------------------------------------------------\n",
      "N:  202000\n",
      "env name:  halfcheetah-medium-replay-v2\n",
      "original n of traj:  202\n",
      "train n of traj:  161\n",
      "val n of traj:  41\n",
      "-----------------------------------------------------\n",
      "N:  2000000\n",
      "env name:  halfcheetah-medium-expert-v2\n",
      "original n of traj:  2000\n",
      "train n of traj:  1600\n",
      "val n of traj:  400\n",
      "-----------------------------------------------------\n",
      "N:  1000000\n",
      "env name:  hopper-medium-v2\n",
      "original n of traj:  2186\n",
      "train n of traj:  1748\n",
      "val n of traj:  438\n",
      "-----------------------------------------------------\n",
      "N:  402000\n",
      "env name:  hopper-medium-replay-v2\n",
      "original n of traj:  2041\n",
      "train n of traj:  1632\n",
      "val n of traj:  409\n",
      "-----------------------------------------------------\n",
      "N:  1999906\n",
      "env name:  hopper-medium-expert-v2\n",
      "original n of traj:  3213\n",
      "train n of traj:  2570\n",
      "val n of traj:  643\n",
      "-----------------------------------------------------\n",
      "N:  1000000\n",
      "env name:  walker2d-medium-v2\n",
      "original n of traj:  1190\n",
      "train n of traj:  952\n",
      "val n of traj:  238\n",
      "-----------------------------------------------------\n",
      "N:  302000\n",
      "env name:  walker2d-medium-replay-v2\n",
      "original n of traj:  1093\n",
      "train n of traj:  874\n",
      "val n of traj:  219\n",
      "-----------------------------------------------------\n",
      "N:  1999995\n",
      "env name:  walker2d-medium-expert-v2\n",
      "original n of traj:  2190\n",
      "train n of traj:  1752\n",
      "val n of traj:  438\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n",
      "pybullet build time: Apr 30 2024 12:01:25\n",
      "c:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\gym\\spaces\\box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\n",
      "load datafile:   0%|          | 0/21 [00:00<?, ?it/s]\n",
      "load datafile:   5%|▍         | 1/21 [00:00<00:04,  4.85it/s]\n",
      "load datafile:  14%|█▍        | 3/21 [00:00<00:04,  4.07it/s]\n",
      "load datafile:  19%|█▉        | 4/21 [00:01<00:05,  3.03it/s]\n",
      "load datafile:  81%|████████  | 17/21 [00:01<00:00, 11.41it/s]\n",
      "load datafile:  86%|████████▌ | 18/21 [00:02<00:00,  7.33it/s]\n",
      "load datafile: 100%|██████████| 21/21 [00:02<00:00,  8.23it/s]\n",
      "\n",
      "load datafile:   0%|          | 0/11 [00:00<?, ?it/s]\n",
      "load datafile:  27%|██▋       | 3/11 [00:00<00:00, 16.86it/s]\n",
      "load datafile:  45%|████▌     | 5/11 [00:00<00:00, 17.34it/s]\n",
      "load datafile:  64%|██████▎   | 7/11 [00:00<00:00, 16.41it/s]\n",
      "load datafile:  82%|████████▏ | 9/11 [00:00<00:00, 12.15it/s]\n",
      "load datafile: 100%|██████████| 11/11 [00:00<00:00, 16.66it/s]\n",
      "\n",
      "load datafile:   0%|          | 0/9 [00:00<?, ?it/s]\n",
      "load datafile:  11%|█         | 1/9 [00:00<00:03,  2.31it/s]\n",
      "load datafile:  33%|███▎      | 3/9 [00:01<00:02,  2.07it/s]\n",
      "load datafile:  44%|████▍     | 4/9 [00:02<00:03,  1.58it/s]\n",
      "load datafile:  56%|█████▌    | 5/9 [00:03<00:03,  1.21it/s]\n",
      "load datafile:  67%|██████▋   | 6/9 [00:04<00:02,  1.04it/s]\n",
      "load datafile: 100%|██████████| 9/9 [00:04<00:00,  1.84it/s]\n",
      "\n",
      "load datafile:   0%|          | 0/21 [00:00<?, ?it/s]\n",
      "load datafile:   5%|▍         | 1/21 [00:00<00:02,  9.69it/s]\n",
      "load datafile:  14%|█▍        | 3/21 [00:00<00:02,  6.46it/s]\n",
      "load datafile:  19%|█▉        | 4/21 [00:00<00:03,  4.75it/s]\n",
      "load datafile:  81%|████████  | 17/21 [00:01<00:00, 17.96it/s]\n",
      "load datafile:  90%|█████████ | 19/21 [00:01<00:00,  9.94it/s]\n",
      "load datafile: 100%|██████████| 21/21 [00:01<00:00, 11.31it/s]\n",
      "\n",
      "load datafile:   0%|          | 0/11 [00:00<?, ?it/s]\n",
      "load datafile:  27%|██▋       | 3/11 [00:00<00:00, 14.95it/s]\n",
      "load datafile:  45%|████▌     | 5/11 [00:00<00:00, 14.78it/s]\n",
      "load datafile:  64%|██████▎   | 7/11 [00:00<00:00, 14.41it/s]\n",
      "load datafile:  82%|████████▏ | 9/11 [00:00<00:00, 13.73it/s]\n",
      "load datafile: 100%|██████████| 11/11 [00:00<00:00, 17.10it/s]\n",
      "\n",
      "load datafile:   0%|          | 0/9 [00:00<?, ?it/s]\n",
      "load datafile:  11%|█         | 1/9 [00:00<00:01,  5.21it/s]\n",
      "load datafile:  33%|███▎      | 3/9 [00:00<00:01,  3.31it/s]\n",
      "load datafile:  44%|████▍     | 4/9 [00:01<00:02,  2.35it/s]\n",
      "load datafile:  56%|█████▌    | 5/9 [00:02<00:02,  1.86it/s]\n",
      "load datafile:  67%|██████▋   | 6/9 [00:03<00:01,  1.67it/s]\n",
      "load datafile: 100%|██████████| 9/9 [00:03<00:00,  2.91it/s]\n",
      "\n",
      "load datafile:   0%|          | 0/21 [00:00<?, ?it/s]\n",
      "load datafile:   5%|▍         | 1/21 [00:00<00:03,  5.19it/s]\n",
      "load datafile:  14%|█▍        | 3/21 [00:00<00:04,  4.18it/s]\n",
      "load datafile:  19%|█▉        | 4/21 [00:09<00:55,  3.24s/it]\n",
      "load datafile:  81%|████████  | 17/21 [00:10<00:01,  2.10it/s]\n",
      "load datafile:  90%|█████████ | 19/21 [00:10<00:00,  2.22it/s]\n",
      "load datafile: 100%|██████████| 21/21 [00:10<00:00,  1.93it/s]\n",
      "\n",
      "load datafile:   0%|          | 0/11 [00:00<?, ?it/s]\n",
      "load datafile:  27%|██▋       | 3/11 [00:00<00:00, 12.28it/s]\n",
      "load datafile:  45%|████▌     | 5/11 [00:00<00:00, 11.43it/s]\n",
      "load datafile:  64%|██████▎   | 7/11 [00:00<00:00, 11.14it/s]\n",
      "load datafile:  82%|████████▏ | 9/11 [00:00<00:00, 10.66it/s]\n",
      "load datafile: 100%|██████████| 11/11 [00:00<00:00, 13.37it/s]\n",
      "\n",
      "load datafile:   0%|          | 0/9 [00:00<?, ?it/s]\n",
      "load datafile:  11%|█         | 1/9 [00:00<00:03,  2.63it/s]\n",
      "load datafile:  33%|███▎      | 3/9 [00:01<00:02,  2.10it/s]\n",
      "load datafile:  44%|████▍     | 4/9 [00:02<00:03,  1.43it/s]\n",
      "load datafile:  56%|█████▌    | 5/9 [00:03<00:03,  1.14it/s]\n",
      "load datafile:  67%|██████▋   | 6/9 [00:05<00:03,  1.02s/it]\n",
      "load datafile: 100%|██████████| 9/9 [00:05<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# data download\n",
    "# if you downloaded, don't re-start.\n",
    "\n",
    "!python ../src/data/download_d4rl_datasets.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment HalfCheetah-v2 is out of date. You should consider upgrading to version `v3` with the environment ID `HalfCheetah-v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# parameter setting\n",
    "\n",
    "env_name = 'halfcheetah'\n",
    "dataset = 'medium-replay'\n",
    "\n",
    "if env_name == 'hopper':\n",
    "    env = gym.make('Hopper-v2')\n",
    "\n",
    "elif env_name == 'halfcheetah':\n",
    "    env = gym.make('HalfCheetah-v2')\n",
    "\n",
    "elif env_name == 'walker2d':\n",
    "    env = gym.make('Walker2d-v2')\n",
    "\n",
    "TRAIN_DATA_PATH = f'{PATH}/transformer/gpt_transformer/src/data/train/{env_name}-{dataset}-v2.pkl'\n",
    "VAL_DATA_PATH = f'{PATH}/transformer/gpt_transformer/src/data/val/{env_name}-{dataset}-v2.pkl'\n",
    "ORIGINAL_DATA_PATH = f'{PATH}/transformer/gpt_transformer/src/data/original/{env_name}-{dataset}-v2.pkl'\n",
    "\n",
    "LOG_PATH = f\"{PATH}/transformer/gpt_transformer/src/log/\"\n",
    "BEST_MODEL_PATH = f\"{PATH}/transformer/gpt_transformer/src/best_model/\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda:0')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\gym\\spaces\\box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "load datafile: 100%|██████████| 11/11 [00:00<00:00, 20.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# env dataset check\n",
    "check_env = gym.make('halfcheetah-medium-replay-v2')\n",
    "check_dataset = check_env.get_dataset()\n",
    "\n",
    "# print(dataset['observations'][1]) # trajectory 단위로 뽑힘.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"overall len: \", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape:  (202000, 17)\n",
      "action shape:  (202000, 6)\n",
      "reward shape:  (202000,)\n",
      "N:  202000\n",
      "train_size:  161600\n"
     ]
    }
   ],
   "source": [
    "print(\"state shape: \", check_dataset['observations'].shape)\n",
    "print(\"action shape: \", check_dataset['actions'].shape)\n",
    "print(\"reward shape: \", check_dataset['rewards'].shape)\n",
    "print(\"N: \", check_dataset['rewards'].shape[0])\n",
    "print(\"train_size: \", int(0.8 * check_dataset['rewards'].shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  [[-4.3362133e-02 -2.6833022e-03  6.5432638e-02 ...  1.0888433e-01\n",
      "   8.8169552e-02  6.3931167e-02]\n",
      " [-3.7674177e-02  1.5365051e-02  2.1673881e-01 ... -8.2050276e+00\n",
      "   2.6932850e+00 -4.4459348e+00]\n",
      " [-8.1146188e-02  1.5728043e-02  2.3461881e-01 ... -1.2342579e+00\n",
      "  -4.2705555e+00 -3.4320550e+00]\n",
      " ...\n",
      " [-5.6956607e-01  3.2922680e+00 -3.1815246e-01 ... -3.0207298e+00\n",
      "   2.2088470e+00 -4.5268812e+00]\n",
      " [-5.7592243e-01  3.3024230e+00 -2.3679866e-01 ...  9.0879889e+00\n",
      "  -1.4607348e+00 -2.9853027e+00]\n",
      " [-5.6932127e-01  3.2941539e+00 -2.2014110e-01 ... -6.1263404e+00\n",
      "   3.7058628e+00  1.0001764e+01]] \n",
      "\n",
      "state:  [[ 6.11328473e-03 -8.39964487e-03  8.37445110e-02 ...  1.55069038e-01\n",
      "  -8.99704620e-02 -3.59976701e-02]\n",
      " [-3.13807465e-03  5.21850809e-02 -3.83801684e-02 ... -4.89395149e-02\n",
      "  -6.29623222e+00  3.82143348e-01]\n",
      " [-1.61008500e-02  1.11764394e-01 -2.30730549e-02 ... -5.31878829e-01\n",
      "   5.92412138e+00  1.48380601e+00]\n",
      " ...\n",
      " [-5.94031950e-03 -1.12105735e-01  4.97356743e-01 ...  5.75514555e+00\n",
      "  -6.00625467e+00  2.44680381e+00]\n",
      " [ 5.51135454e-04 -1.44756466e-01  2.54730046e-01 ... -4.85322380e+00\n",
      "   8.73625088e+00  4.67099333e+00]\n",
      " [-2.91598272e-02 -1.72371611e-01  9.81497541e-02 ...  2.56775594e+00\n",
      "  -6.78099632e+00  5.48049784e+00]] \n",
      "\n",
      "state:  [[ 2.2889530e-02  1.7415540e-03  4.7035575e-02 ... -1.0522804e-02\n",
      "   1.0840961e-01 -4.5108292e-02]\n",
      " [ 1.8400282e-03 -3.4414555e-04  1.3544355e-01 ...  5.7661138e+00\n",
      "  -3.6255813e+00 -7.1817231e+00]\n",
      " [-5.2844606e-02  1.0057127e-04 -1.1423600e-01 ... -6.0992732e+00\n",
      "  -1.9474143e+00  1.0878069e+00]\n",
      " ...\n",
      " [-7.0468642e-02  6.8532087e-02  3.6150295e-02 ...  6.7243404e+00\n",
      "   1.0332606e+01  5.3768964e+00]\n",
      " [-1.2258561e-01  1.0851196e-01 -1.9798449e-01 ...  9.8568511e-01\n",
      "  -4.4059639e+00 -4.5456311e-01]\n",
      " [-1.7543773e-01  1.5208817e-01  1.4988193e-01 ...  6.1975784e+00\n",
      "  -5.0133834e+00 -4.1430249e+00]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data check\n",
    "# check trajectories\n",
    "\n",
    "with open(ORIGINAL_DATA_PATH, 'rb') as f:\n",
    "    trajectories = pickle.load(f)\n",
    "n=0\n",
    "max_rewards_list = []\n",
    "for traj in trajectories:\n",
    "    # print(f\"{n+1}번째 trajectory\")\n",
    "    # print(\"traj: \", traj)\n",
    "    print(\"state: \", traj['observations'], \"\\n\")\n",
    "    # print(\"action: \", traj['actions'], \"\\n\")\n",
    "    # print(\"next_state: \", traj['next_observations'], \"\\n\")\n",
    "    # print(\"reward: \", traj['rewards'], \"\\n\")\n",
    "    # print(\"max_rewards: \", max(traj['rewards']))\n",
    "    # max_rewards_list.append(max(traj['rewards']))\n",
    "    # print(\"\")\n",
    "    n+=1\n",
    "    \n",
    "# print(max(max_rewards_list))\n",
    "\n",
    "    if n==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  161000\n",
      "n of epi:  161\n",
      "n of traj in one epi:  1000\n"
     ]
    }
   ],
   "source": [
    "# check train data shape\n",
    "with open(TRAIN_DATA_PATH, 'rb') as f:\n",
    "    train_trajectories = pickle.load(f)\n",
    "\n",
    "print(\"length: \", len(train_trajectories)*len(train_trajectories[0]['observations']))\n",
    "print(\"n of epi: \", len(train_trajectories))\n",
    "print(\"n of traj in one epi: \", len(train_trajectories[0]['observations']))\n",
    "# print(\"train state shape: \", train_trajectories['observations'].shape)\n",
    "# print(\"train action shape: \", train_trajectories['actions'].shape)\n",
    "# print(\"train reward shape: \", train_trajectories['rewards'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ori: [[-4.3362133e-02 -2.6833022e-03  6.5432638e-02 ...  1.0888433e-01\n",
      "   8.8169552e-02  6.3931167e-02]\n",
      " [-3.7674177e-02  1.5365051e-02  2.1673881e-01 ... -8.2050276e+00\n",
      "   2.6932850e+00 -4.4459348e+00]\n",
      " [-8.1146188e-02  1.5728043e-02  2.3461881e-01 ... -1.2342579e+00\n",
      "  -4.2705555e+00 -3.4320550e+00]\n",
      " ...\n",
      " [-5.6956607e-01  3.2922680e+00 -3.1815246e-01 ... -3.0207298e+00\n",
      "   2.2088470e+00 -4.5268812e+00]\n",
      " [-5.7592243e-01  3.3024230e+00 -2.3679866e-01 ...  9.0879889e+00\n",
      "  -1.4607348e+00 -2.9853027e+00]\n",
      " [-5.6932127e-01  3.2941539e+00 -2.2014110e-01 ... -6.1263404e+00\n",
      "   3.7058628e+00  1.0001764e+01]]\n",
      "train: [[-1.1576745e-02  2.9706063e-02 -4.3819766e-02 ... -1.5118380e-01\n",
      "   2.1798883e-02  8.2527779e-02]\n",
      " [-2.4950452e-02  1.0053803e-02 -3.0048784e-02 ...  2.8199830e+00\n",
      "   2.1435523e+00 -7.0969844e-01]\n",
      " [-5.7788070e-02 -2.7738808e-04  1.7422727e-01 ... -2.3943918e+00\n",
      "  -2.0879815e+00  7.5078583e+00]\n",
      " ...\n",
      " [-1.4442080e-01 -1.4363490e-01 -5.4363328e-01 ... -8.4365475e-01\n",
      "   6.1477447e-01  1.0531671e+01]\n",
      " [-1.1768670e-01 -2.8385791e-01 -4.2025939e-01 ... -1.3004768e+01\n",
      "  -3.7555509e+00 -2.1042500e+00]\n",
      " [-6.4016491e-02 -2.3875797e-01 -8.0064647e-02 ... -1.6853371e+01\n",
      "   3.9323945e+00 -9.0523243e+00]]\n"
     ]
    }
   ],
   "source": [
    "# compare original and train -> check shuffle\n",
    "print(\"ori:\", trajectories[0]['observations'])\n",
    "print(\"train:\", train_trajectories[0]['observations'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': [9, 0], 'b': [19, 10], 'c': [29, 20], 'd': [39, 30]}, {'a': [1, 2], 'b': [11, 12], 'c': [21, 22], 'd': [31, 32]}, {'a': [7, 8], 'b': [17, 18], 'c': [27, 28], 'd': [37, 38]}, {'a': [5, 6], 'b': [15, 16], 'c': [25, 26], 'd': [35, 36]}, {'a': [3, 4], 'b': [13, 14], 'c': [23, 24], 'd': [33, 34]}]\n"
     ]
    }
   ],
   "source": [
    "# check shuffle\n",
    "\n",
    "array = [{'a': [1,2], 'b': [11,12], 'c': [21,22], 'd': [31,32]}, \\\n",
    "        {'a': [3,4], 'b': [13,14], 'c': [23,24], 'd': [33,34]}, \\\n",
    "        {'a': [5,6], 'b': [15,16], 'c': [25,26], 'd': [35,36]}, \\\n",
    "        {'a': [7,8], 'b': [17,18], 'c': [27,28], 'd': [37,38]}, \\\n",
    "        {'a': [9,0], 'b': [19,10], 'c': [29,20], 'd': [39,30]}]\n",
    "        \n",
    "np.random.shuffle(array)\n",
    "\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  41000\n",
      "n of epi:  41\n",
      "n of traj in one epi:  1000\n"
     ]
    }
   ],
   "source": [
    "# check valid data shape\n",
    "with open(VAL_DATA_PATH, 'rb') as f:\n",
    "    val_trajectories = pickle.load(f)\n",
    "\n",
    "print(\"length: \", len(val_trajectories)*len(val_trajectories[0]['observations']))\n",
    "print(\"n of epi: \", len(val_trajectories))\n",
    "print(\"n of traj in one epi: \", len(val_trajectories[0]['observations']))\n",
    "# print(\"val state shape: \", val_trajectories['observations'].shape)\n",
    "# print(\"val action shape: \", val_trajectories['actions'].shape)\n",
    "# print(\"val reward shape: \", val_trajectories['rewards'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train parameter\n",
    "batch_size = 128\n",
    "embed_dim = 128\n",
    "activation = 'relu'\n",
    "drop_out = 0.1\n",
    "k = 31 # content len\n",
    "n_blocks = 3\n",
    "n_heads = 1 # transformer head\n",
    "\n",
    "# total updates = max_train_iters x num_updates_per_iter\n",
    "max_train_iters = 1000\n",
    "num_updates_per_iter = 100\n",
    "# num_val_iter = 100\n",
    "total_updates = 0\n",
    "min_total_log_loss = 1e10\n",
    "\n",
    "wt_decay = 1e-4             # weight decay\n",
    "lr = 1e-4                   # learning rate\n",
    "warmup_steps = 10000        # warmup steps for lr scheduler\n",
    "\n",
    "# weight of mse loss\n",
    "state_weight = 1\n",
    "reward_weight = 1\n",
    "\n",
    "# evaluation parameter\n",
    "# max_eval_ep_len = 1000      # max len of one evaluation episode\n",
    "# num_eval_ep = 10            # num of evaluation episodes per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dim:  17\n",
      "action dim:  6\n"
     ]
    }
   ],
   "source": [
    "# check dim\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "print(\"state dim: \", state_dim)\n",
    "print(\"action dim: \", act_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data loader tester\n",
    "# test_traj_dataset = D4RLTrajectoryDataset(TRAIN_DATA_PATH, k)\n",
    "# test_traj_data_loader = DataLoader(test_traj_dataset,\n",
    "# \t\t\t\t\t\tbatch_size=batch_size,\n",
    "# \t\t\t\t\t\tshuffle=True,\n",
    "# \t\t\t\t\t\tpin_memory=True,\n",
    "# \t\t\t\t\t\tdrop_last=True)\n",
    "                        \n",
    "# test_data_iter = iter(test_traj_data_loader)\n",
    "\n",
    "\n",
    "# for i_train_iter in tqdm(range(max_train_iters)):\n",
    "\t\n",
    "# \tfor _ in range(num_updates_per_iter):\n",
    "# \t\ttry:\n",
    "# \t\t\ttimesteps, states, next_states, actions, rewards, traj_mask = next(test_data_iter)\n",
    "# \t\texcept StopIteration:\n",
    "# \t\t\ttest_traj_data_loader = DataLoader(test_traj_dataset,\n",
    "# \t\t\t\t\t\t\t\t\tbatch_size=batch_size,\n",
    "# \t\t\t\t\t\t\t\t\tshuffle=True,\n",
    "# \t\t\t\t\t\t\t\t\tpin_memory=True,\n",
    "# \t\t\t\t\t\t\t\t\tdrop_last=True)\n",
    "# \t\t\ttest_data_iter = iter(test_traj_data_loader)\n",
    "# \t\t\ttimesteps, states, next_states, actions, rewards, traj_mask = next(test_data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 32\n"
     ]
    }
   ],
   "source": [
    "# # load validate preprocessing(normalization, fit padding) data\n",
    "\n",
    "val_traj_dataset = D4RLTrajectoryDataset(TRAIN_DATA_PATH, k, val=True, val_dataset_path=VAL_DATA_PATH)\n",
    "\n",
    "batch_size = check_batch(batch_size, len(val_traj_dataset))\n",
    "\n",
    "print(\"batch_size:\", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train preprocessing(normalization, fit padding) data\n",
    "\n",
    "train_traj_dataset = D4RLTrajectoryDataset(TRAIN_DATA_PATH, k)\n",
    "train_traj_data_loader = DataLoader(train_traj_dataset,\n",
    "\t\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\tdrop_last=True)\n",
    "                        \n",
    "train_data_iter = iter(train_traj_data_loader)\n",
    "\n",
    "## get state stats from dataset\n",
    "state_mean, state_std = train_traj_dataset.get_state_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "model = DecisionTransformer(\n",
    "\t\t\tstate_dim=state_dim,\n",
    "\t\t\tact_dim=act_dim,\n",
    "\t\t\tn_blocks=n_blocks,\n",
    "\t\t\th_dim=embed_dim,\n",
    "\t\t\tcontext_len=k,\n",
    "\t\t\tn_heads=n_heads,\n",
    "\t\t\tdrop_p=drop_out,\n",
    "\t\t).to(DEVICE)\n",
    "  \n",
    "optimizer = torch.optim.AdamW(\n",
    "\t\t\t\t\tmodel.parameters(), \n",
    "\t\t\t\t\tlr=lr, \n",
    "\t\t\t\t\tweight_decay=wt_decay\n",
    "\t\t\t\t)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "\t\toptimizer,\n",
    "\t\tlambda steps: min((steps+1)/warmup_steps, 1)\n",
    "\t)\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now().replace(microsecond=0)\n",
    "\n",
    "start_time_str = start_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "prefix = f\"{env_name}-{dataset}\"\n",
    "\n",
    "save_model_name =  f'{prefix}_model.pt'\n",
    "save_best_model_name = f'{prefix}_model_best.pt'\n",
    "save_model_path = os.path.join(LOG_PATH, save_model_name)\n",
    "save_best_model_path = os.path.join(BEST_MODEL_PATH, save_best_model_name)\n",
    "\n",
    "log_csv_name = prefix + \"_log_\" + start_time_str + \".csv\"\n",
    "log_csv_path = os.path.join(LOG_PATH, log_csv_name)\n",
    "\n",
    "\n",
    "csv_writer = csv.writer(open(log_csv_path, 'a', 1))\n",
    "csv_header = ([\"duration\", \"num_updates\", \"total_loss\", \"state_loss\", \"reward_loss\", \"val_total_loss\", \"val_state_loss\", \"val_reward_loss\"])\n",
    "\n",
    "csv_writer.writerow(csv_header)\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"start time: \" + start_time_str)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"device set to: \" + str(DEVICE))\n",
    "print(\"dataset: \" + prefix)\n",
    "print(\"batch_size: \" + str(batch_size))\n",
    "print(\"best model save path: \" + save_best_model_path)\n",
    "print(\"log csv save path: \" + log_csv_path)\n",
    "\n",
    "# train\n",
    "for i_train_iter in tqdm(range(max_train_iters)):\n",
    "\n",
    "\n",
    "\tlog_state_losses, log_reward_losses, log_total_losses = [], [], []\n",
    "\tval_log_state_losses, val_log_reward_losses, val_log_total_losses = [], [], []\n",
    "\tmodel.train()\n",
    "\t\n",
    "\tfor _ in range(num_updates_per_iter):\n",
    "\t\ttry:\n",
    "\t\t\ttimesteps, states, next_states, actions, rewards, traj_mask = next(train_data_iter)\n",
    "\t\texcept StopIteration:\n",
    "\t\t\ttrain_traj_data_loader = DataLoader(train_traj_dataset,\n",
    "\t\t\t\t\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\t\t\t\tdrop_last=True)\n",
    "\t\t\ttrain_data_iter = iter(train_traj_data_loader)\n",
    "\t\t\ttimesteps, states, next_states, actions, rewards, traj_mask = next(train_data_iter)\n",
    "\n",
    "\t\ttimesteps = timesteps.to(DEVICE)\t# B x T\n",
    "\t\tstates = states.to(DEVICE)\t\t\t# B x T x state_dim\n",
    "\t\tnext_states = next_states.to(DEVICE) # B X T X state_dim\n",
    "\t\tactions = actions.to(DEVICE)\t\t# B x T x act_dim\n",
    "\t\trewards = rewards.to(DEVICE).unsqueeze(dim=-1) # B x T x 1\n",
    "\t\ttraj_mask = traj_mask.to(DEVICE)\t# B x T\n",
    "\n",
    "\t\tnext_states_target = torch.clone(next_states).detach().to(DEVICE)\n",
    "\t\trewards_target = torch.clone(rewards).detach().to(DEVICE)\n",
    "\t\n",
    "\t\tnext_state_preds, rewards_preds = model.forward(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\ttimesteps=timesteps,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tstates=states,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tactions=actions,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\trewards=rewards,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\t\t# only consider non padded elements\n",
    "\t\tnext_state_preds = next_state_preds.view(-1, state_dim)[traj_mask.view(-1,) > 0]\n",
    "\t\tnext_states_target = next_states_target.view(-1, state_dim)[traj_mask.view(-1,) > 0]\n",
    "\t\t\n",
    "\t\trewards_preds = rewards_preds.view(-1, 1)[traj_mask.view(-1,) > 0]\n",
    "\t\trewards_target = rewards_target.view(-1, 1)[traj_mask.view(-1,) > 0]\n",
    "\n",
    "\t\tstate_loss = F.mse_loss(next_state_preds, next_states_target, reduction='mean') * state_weight\n",
    "\t\treward_loss = F.mse_loss(rewards_preds, rewards_target, reduction='mean') * reward_weight\n",
    "\t\t\n",
    "\t\ttotal_loss = state_loss.add(reward_loss)\n",
    "\t\ttotal_loss = torch.mean(total_loss)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\ttotal_loss.backward()\n",
    "\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "\t\toptimizer.step()\n",
    "\t\tscheduler.step()\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t#save loss\n",
    "\t\tlog_state_losses.append(state_loss.detach().cpu().item())\n",
    "\t\tlog_reward_losses.append(reward_loss.detach().cpu().item())\n",
    "\t\t\n",
    "\t\tlog_total_losses.append(total_loss.detach().cpu().item())\n",
    "\t\t\n",
    "\t# validation\n",
    "\tmodel.eval()\n",
    "\tval_traj_data_loader = DataLoader(val_traj_dataset,\n",
    "\t\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\tdrop_last=True)\n",
    "\tfor val_timesteps, val_states, val_next_states, val_actions, val_rewards, val_traj_mask in val_traj_data_loader:\n",
    "\t\t\n",
    "\t\tval_timesteps = val_timesteps.to(DEVICE)\t# B x T\n",
    "\t\tval_states = val_states.to(DEVICE)\t\t\t# B x T x state_dim\n",
    "\t\tval_next_states = val_next_states.to(DEVICE) # B X T X state_dim\n",
    "\t\tval_actions = val_actions.to(DEVICE)\t\t# B x T x act_dim\n",
    "\t\tval_rewards = val_rewards.to(DEVICE).unsqueeze(dim=-1) # B x T x 1\n",
    "\t\tval_traj_mask = val_traj_mask.to(DEVICE)\t# B x T\n",
    "\t\t\t\t\n",
    "\t\tval_next_states_target = torch.clone(val_next_states).detach().to(DEVICE)\n",
    "\t\tval_rewards_target = torch.clone(val_rewards).detach().to(DEVICE)\n",
    "\t\t\n",
    "\t\tval_next_state_preds, val_rewards_preds = model.forward(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\ttimesteps=val_timesteps,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tstates=val_states,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tactions=val_actions,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\trewards=val_rewards,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t# only consider non padded elements\n",
    "\t\tval_next_state_preds = val_next_state_preds.view(-1, state_dim)[val_traj_mask.view(-1,) > 0]\n",
    "\t\tval_next_states_target = val_next_states_target.view(-1, state_dim)[val_traj_mask.view(-1,) > 0]\n",
    "\t\t\n",
    "\t\tval_rewards_preds = val_rewards_preds.view(-1, 1)[val_traj_mask.view(-1,) > 0]\n",
    "\t\tval_rewards_target = val_rewards_target.view(-1, 1)[val_traj_mask.view(-1,) > 0]\n",
    "\n",
    "\t\tval_state_loss = F.mse_loss(val_next_state_preds, val_next_states_target, reduction='mean') * state_weight\n",
    "\t\tval_reward_loss = F.mse_loss(val_rewards_preds, val_rewards_target, reduction='mean') * reward_weight\n",
    "\n",
    "\t\t# todo: try to use mae\n",
    "\t\t\n",
    "\t\tval_total_loss = val_state_loss.add(val_reward_loss)\n",
    "\t\tval_total_loss = torch.mean(val_total_loss)\n",
    "\t\t\n",
    "\t\t# save val loss\n",
    "\t\tval_log_state_losses.append(val_state_loss.detach().cpu().item())\n",
    "\t\tval_log_reward_losses.append(val_reward_loss.detach().cpu().item())\n",
    "\t\t\n",
    "\t\tval_log_total_losses.append(val_total_loss.detach().cpu().item())\n",
    "\t\n",
    "\tmean_total_log_loss = np.mean(log_total_losses)\n",
    "\tmean_state_log_loss = np.mean(log_state_losses)\n",
    "\tmean_reward_log_loss = np.mean(log_reward_losses)\n",
    "\t\n",
    "\tmean_val_total_log_loss = np.mean(val_log_total_losses)\n",
    "\tmean_val_state_log_loss = np.mean(val_log_state_losses)\n",
    "\tmean_val_reward_log_loss = np.mean(val_log_reward_losses)\n",
    "\n",
    "\ttime_elapsed = str(datetime.now().replace(microsecond=0) - start_time)\n",
    "\n",
    "\ttotal_updates += num_updates_per_iter\n",
    "\n",
    "\tlog_str = (\"=\" * 60 + '\\n' +\n",
    "\t\t\t\"time elapsed: \" + time_elapsed  + '\\n' +\n",
    "\t\t\t\"num of updates: \" + str(total_updates) + '\\n' +\n",
    "\t\t\t\"train total loss: \" + format(mean_total_log_loss, \".5f\") + '\\n' +\n",
    "\t\t\t\"train state loss: \" + format(mean_state_log_loss, \".5f\") + '\\n' +\n",
    "\t\t\t\"train reward loss: \" +  format(mean_reward_log_loss, \".5f\") + '\\n' +\n",
    "\t\t\t\"val total loss: \" + format(mean_val_total_log_loss, \".5f\") + '\\n' +\n",
    "\t\t\t\"val state loss: \" + format(mean_val_state_log_loss, \".5f\") + '\\n' +\n",
    "\t\t\t\"val reward loss: \" +  format(mean_val_reward_log_loss, \".5f\")\n",
    "\t\t\t)\n",
    "\n",
    "\tprint(log_str)\n",
    "\n",
    "\tlog_data = [time_elapsed, total_updates, mean_total_log_loss, mean_state_log_loss, mean_reward_log_loss, \\\n",
    "\t\t mean_val_total_log_loss, mean_val_state_log_loss, mean_val_reward_log_loss]\n",
    "\n",
    "\tcsv_writer.writerow(log_data)\n",
    "\t\n",
    "\t# save model\n",
    "\tif mean_val_total_log_loss <= min_total_log_loss:\n",
    "\t\tprint(\"saving min loss model at: \" + save_best_model_path)\n",
    "\t\ttorch.save(model.state_dict(), save_best_model_path)\n",
    "\t\tmin_total_log_loss = mean_val_total_log_loss\n",
    "\n",
    "\tprint(\"saving current model at: \" + save_model_path)\n",
    "\ttorch.save(model.state_dict(), save_model_path)\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"finished training!\")\n",
    "print(\"=\" * 60)\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "time_elapsed = str(end_time - start_time)\n",
    "end_time_str = end_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "print(\"started training at: \" + start_time_str)\n",
    "print(\"finished training at: \" + end_time_str)\n",
    "print(\"total training time: \" + time_elapsed)\n",
    "print(\"saved best model at: \" + save_best_model_path)\n",
    "print(\"saved last updated model at: \" + save_model_path)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('rl_diffusion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aaa54a55e816925fdb1964dae2307e16b21383867fc9aac098dfe4376e4c067b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
