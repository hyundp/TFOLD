{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import d4rl\n",
    "import gym\n",
    "import numpy as np\n",
    "import collections\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import D4RLTrajectoryDataset\n",
    "from model import DecisionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment\n",
    "# sys.path.append(r'C:\\Develop\\offlineRL-with-diffusion') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mujoco-py check passed\n",
      "d4rl check passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n",
      "pybullet build time: Apr 30 2024 12:01:25\n",
      "c:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\gym\\spaces\\box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "# test mujoco, d4rl\n",
    "\n",
    "!python ./test/mujoco_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data download\n",
    "# if you downloaded, don't re-start.\n",
    "\n",
    "# !python ./data/download_d4rl_datasets.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment HalfCheetah-v2 is out of date. You should consider upgrading to version `v3` with the environment ID `HalfCheetah-v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# parameter setting\n",
    "\n",
    "env_name = 'halfcheetah'\n",
    "dataset = 'medium'\n",
    "\n",
    "if env_name == 'hopper':\n",
    "    env = gym.make('Hopper-v2')\n",
    "    max_ep_len = 1000\n",
    "    # env_targets = [3600, 1800]  # evaluation conditioning targets\n",
    "    # scale = 1000.  # normalization for rewards/returns\n",
    "elif env_name == 'halfcheetah':\n",
    "    env = gym.make('HalfCheetah-v2')\n",
    "    max_ep_len = 1000\n",
    "    # env_targets = [12000, 6000]\n",
    "    # scale = 1000.\n",
    "elif env_name == 'walker2d':\n",
    "    env = gym.make('Walker2d-v2')\n",
    "    max_ep_len = 1000\n",
    "    # env_targets = [5000, 2500]\n",
    "    # scale = 1000.\n",
    "\n",
    "DATA_PATH = f'data/train/{env_name}-{dataset}-v2.pkl'\n",
    "VAL_DATA_PATH = f'data/val/val_{env_name}-{dataset}-v2.pkl'\n",
    "TEMP_DATA_PATH = f'data/temp/{env_name}-{dataset}-v2.pkl'\n",
    "LOG_PATH = \"./log/\"\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda:0')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\gym\\spaces\\box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "load datafile: 100%|██████████| 21/21 [00:02<00:00,  8.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# env dataset check\n",
    "check_env = gym.make('halfcheetah-medium-v2')\n",
    "dataset = check_env.get_dataset()\n",
    "\n",
    "# print(dataset['observations'][1]) # trajectory 단위로 뽑힘.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"overall len: \", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape:  (1000000, 17)\n",
      "action shape:  (1000000, 6)\n",
      "reward shape:  (1000000,)\n",
      "N:  1000000\n",
      "train_size:  800000\n"
     ]
    }
   ],
   "source": [
    "print(\"state shape: \", dataset['observations'].shape)\n",
    "print(\"action shape: \", dataset['actions'].shape)\n",
    "print(\"reward shape: \", dataset['rewards'].shape)\n",
    "print(\"N: \", dataset['rewards'].shape[0])\n",
    "print(\"train_size: \", int(0.8 * dataset['rewards'].shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  [[ 1.9831914e-02 -8.9501314e-02 -3.1969063e-03 ...  1.1365079e-01\n",
      "   6.8424918e-02 -1.3811582e-01]\n",
      " [-3.8486063e-03 -5.2394319e-02  8.3050327e-03 ...  4.5068407e+00\n",
      "  -9.2885571e+00  4.7328596e+00]\n",
      " [-5.5298433e-02 -7.7850236e-05 -2.3952831e-01 ... -7.0811687e+00\n",
      "  -1.4037068e+00  7.5524049e+00]\n",
      " ...\n",
      " [-3.1975684e-01  5.3305399e-01 -4.8704177e-01 ...  1.5455554e+00\n",
      "   2.6812897e+00  8.7905388e+00]\n",
      " [-3.2200974e-01  3.5745117e-01  1.0463273e-02 ... -6.3428599e-01\n",
      "   1.6292539e+00  9.7356015e-01]\n",
      " [-3.0673215e-01  1.9843711e-01  6.9996923e-01 ...  5.0098950e-01\n",
      "   1.5680059e+00  9.4733723e-02]] \n",
      "\n",
      "state:  [[ 4.7026437e-02 -2.1588113e-02  4.9151547e-02 ...  5.5219561e-02\n",
      "  -1.5351681e-01 -4.6239123e-02]\n",
      " [ 4.1392505e-02  5.3802542e-02 -1.5022255e-01 ...  6.1133021e-01\n",
      "  -7.4645710e+00  7.9509692e+00]\n",
      " [ 9.8547200e-04  8.8533267e-02 -4.3876743e-01 ...  8.5824745e-04\n",
      "   5.9796906e+00  4.9521341e+00]\n",
      " ...\n",
      " [-1.4081973e-01 -7.7957302e-02 -2.6429656e-01 ...  1.0316861e+00\n",
      "  -7.5645506e-01 -6.3285580e+00]\n",
      " [-6.0340445e-02 -4.2597357e-02  1.1770165e-01 ... -1.8022682e+01\n",
      "  -2.8895503e-01 -8.5740490e+00]\n",
      " [-3.9826483e-02  8.9701705e-02  4.8491257e-01 ... -1.5949978e+01\n",
      "  -4.2975497e+00  3.4759271e-01]] \n",
      "\n",
      "state:  [[ 5.0221119e-02 -2.4338657e-02 -5.4945849e-02 ... -1.4283662e-01\n",
      "  -1.2819463e-01 -1.4791407e-01]\n",
      " [ 3.4841169e-02  2.6000816e-02 -2.3993976e-01 ... -2.5026150e+00\n",
      "  -9.1679888e+00  6.2272105e+00]\n",
      " [-2.2413606e-02  5.1811527e-02 -5.4271245e-01 ... -5.9999785e+00\n",
      "  -6.6550332e-01 -2.4636595e-01]\n",
      " ...\n",
      " [-4.4023961e-02 -2.5939721e-01 -7.0705950e-02 ... -1.8107307e+01\n",
      "  -4.7313199e+00 -4.8486176e+00]\n",
      " [-1.7510138e-02 -2.1372417e-01  2.7898479e-02 ...  2.9009397e+00\n",
      "  -2.2113817e+00 -5.9028540e+00]\n",
      " [ 4.7847845e-02 -1.5627538e-01 -1.8005599e-01 ...  9.9821644e+00\n",
      "  -5.1808944e+00  1.1870988e+01]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data check\n",
    "# check three trajectories\n",
    "\n",
    "with open(DATA_PATH, 'rb') as f:\n",
    "    trajectories = pickle.load(f)\n",
    "n=0\n",
    "max_rewards_list = []\n",
    "for traj in trajectories:\n",
    "    # print(f\"{n+1}번째 trajectory\")\n",
    "    # print(\"traj: \", traj)\n",
    "    print(\"state: \", traj['observations'], \"\\n\")\n",
    "    # print(\"action: \", traj['actions'], \"\\n\")\n",
    "    # print(\"next_state: \", traj['next_observations'], \"\\n\")\n",
    "    # print(\"reward: \", traj['rewards'], \"\\n\")\n",
    "    # print(\"max_rewards: \", max(traj['rewards']))\n",
    "    # max_rewards_list.append(max(traj['rewards']))\n",
    "    # print(\"\")\n",
    "    n+=1\n",
    "    \n",
    "# print(max(max_rewards_list))\n",
    "\n",
    "    if n==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  1000000\n",
      "n of epi:  1000\n",
      "n of traj in one epi:  1000\n"
     ]
    }
   ],
   "source": [
    "# # check original data shape\n",
    "# with open(TEMP_DATA_PATH, 'rb') as f:\n",
    "#     temp_trajectories = pickle.load(f)\n",
    "    \n",
    "# print(\"length: \", len(temp_trajectories)*len(temp_trajectories[0]['observations']))\n",
    "# print(\"n of epi: \", len(temp_trajectories))\n",
    "# print(\"n of traj in one epi: \", len(temp_trajectories[0]['observations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 17)\n",
      "(1000, 17)\n",
      "(1000, 17)\n",
      "(1000, 17)\n",
      "(1000, 17)\n",
      "(1000, 17)\n"
     ]
    }
   ],
   "source": [
    "# # check original dataset\n",
    "# cnt = 0\n",
    "\n",
    "\n",
    "# for ori in temp_trajectories:\n",
    "#     print(ori['observations'].shape)\n",
    "    \n",
    "#     if cnt >= 5:\n",
    "#         break\n",
    "    \n",
    "#     cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape:  (1000000, 17)\n"
     ]
    }
   ],
   "source": [
    "# states, next_states, rewards = [], [], []\n",
    "# for traj in temp_trajectories:\n",
    "#     # print(traj)\n",
    "#     traj_len = traj['observations'].shape[0]\n",
    "#     states.append(traj['observations'])\n",
    "#     next_states.append(traj['next_observations'])\n",
    "#     rewards.append(traj['rewards'])\n",
    "#     # # calculate returns to go and rescale them\n",
    "#     # traj['returns_to_go'] = discount_cumsum(traj['rewards'], 1.0) / rtg_scale\n",
    "    \n",
    "# states = np.concatenate(states, axis=0)\n",
    "# print(\"state shape: \", states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  800000\n",
      "n of epi:  800\n",
      "n of traj in one epi:  1000\n"
     ]
    }
   ],
   "source": [
    "# check train data shape\n",
    "with open(DATA_PATH, 'rb') as f:\n",
    "    train_trajectories = pickle.load(f)\n",
    "\n",
    "print(\"length: \", len(train_trajectories)*len(train_trajectories[0]['observations']))\n",
    "print(\"n of epi: \", len(train_trajectories))\n",
    "print(\"n of traj in one epi: \", len(train_trajectories[0]['observations']))\n",
    "# print(\"train state shape: \", train_trajectories['observations'].shape)\n",
    "# print(\"train action shape: \", train_trajectories['actions'].shape)\n",
    "# print(\"train reward shape: \", train_trajectories['rewards'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  200000\n",
      "n of epi:  200\n",
      "n of traj in one epi:  1000\n"
     ]
    }
   ],
   "source": [
    "# check valid data shape\n",
    "with open(VAL_DATA_PATH, 'rb') as f:\n",
    "    val_trajectories = pickle.load(f)\n",
    "\n",
    "print(\"length: \", len(val_trajectories)*len(val_trajectories[0]['observations']))\n",
    "print(\"n of epi: \", len(val_trajectories))\n",
    "print(\"n of traj in one epi: \", len(val_trajectories[0]['observations']))\n",
    "# print(\"val state shape: \", val_trajectories['observations'].shape)\n",
    "# print(\"val action shape: \", val_trajectories['actions'].shape)\n",
    "# print(\"val reward shape: \", val_trajectories['rewards'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train parameter\n",
    "batch_size = 64\n",
    "embed_dim = 128\n",
    "activation = 'relu'\n",
    "drop_out = 0.1\n",
    "k = 31 # content len\n",
    "n_blocks = 3\n",
    "n_heads = 1 # transformer head\n",
    "\n",
    "# total updates = max_train_iters x num_updates_per_iter\n",
    "max_train_iters = 400\n",
    "num_updates_per_iter = 100\n",
    "# num_val_iter = 100\n",
    "total_updates = 0\n",
    "min_total_log_loss = 1e10\n",
    "\n",
    "wt_decay = 1e-4             # weight decay\n",
    "lr = 1e-4                   # learning rate\n",
    "warmup_steps = 10000        # warmup steps for lr scheduler\n",
    "\n",
    "# weight of mse loss\n",
    "state_weight = 1\n",
    "reward_weight = 1\n",
    "\n",
    "# evaluation parameter\n",
    "# max_eval_ep_len = 1000      # max len of one evaluation episode\n",
    "# num_eval_ep = 10            # num of evaluation episodes per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dim:  17\n",
      "action dim:  6\n"
     ]
    }
   ],
   "source": [
    "# check dim\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "print(\"state dim: \", state_dim)\n",
    "print(\"action dim: \", act_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps shape:  torch.Size([32, 2])\n",
      "rewards shape:  torch.Size([32, 2, 1])\n",
      "states shape:  torch.Size([32, 2, 17])\n",
      "actions shape:  torch.Size([32, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "temp_dataset = D4RLTrajectoryDataset(DATA_PATH, 2)\n",
    "temp_data_loader = DataLoader(temp_dataset,\n",
    "\t\t\t\t\t\tbatch_size=32,\n",
    "\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\tdrop_last=True)\n",
    "                        \n",
    "temp_data_iter = iter(temp_data_loader)\n",
    "\n",
    "timesteps, states, next_states, actions, rewards, traj_mask = next(temp_data_iter)\n",
    "\n",
    "timesteps = timesteps.to(DEVICE)\t# B x T\n",
    "states = states.to(DEVICE)\t\t\t# B x T x state_dim\n",
    "next_states = next_states.to(DEVICE) # B X T X state_dim\n",
    "actions = actions.to(DEVICE)\t\t# B x T x act_dim\n",
    "rewards = rewards.to(DEVICE).unsqueeze(dim=-1) # B x T x 1\n",
    "\n",
    "print(\"timesteps shape: \", timesteps.shape)\n",
    "print(\"rewards shape: \", rewards.shape)\n",
    "print(\"states shape: \", states.shape)\n",
    "print(\"actions shape: \", actions.shape)\n",
    "\n",
    "# print(\"state: \", states)\n",
    "# print(\"action: \", actions)\n",
    "# print(\"rewards: \", rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "\n",
    "temp_model = DecisionTransformer(\n",
    "\t\t\tstate_dim=state_dim,\n",
    "\t\t\tact_dim=act_dim,\n",
    "\t\t\t# reward 포함 + r0 제외\n",
    "\t\t\tn_blocks=n_blocks,\n",
    "\t\t\th_dim=16,\n",
    "\t\t\tcontext_len=2,\n",
    "\t\t\tn_heads=n_heads,\n",
    "\t\t\tdrop_p=drop_out,\n",
    "\t\t).to(DEVICE)\n",
    "\t\t\n",
    "next_state_preds, rewards_preds = temp_model.forward(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\trewards=rewards,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ttimesteps=timesteps,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tstates=states,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tactions=actions,\n",
    "\t\t\t\t\t\t\t\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:34<00:00,  4.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# continue train test\n",
    "test_traj_dataset = D4RLTrajectoryDataset(DATA_PATH, k)\n",
    "test_traj_data_loader = DataLoader(test_traj_dataset,\n",
    "\t\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\tdrop_last=True)\n",
    "                        \n",
    "test_data_iter = iter(test_traj_data_loader)\n",
    "\n",
    "\n",
    "for i_train_iter in tqdm(range(max_train_iters)):\n",
    "\t\n",
    "\tfor _ in range(num_updates_per_iter):\n",
    "\t\ttry:\n",
    "\t\t\ttimesteps, states, next_states, actions, rewards, traj_mask = next(test_data_iter)\n",
    "\t\texcept StopIteration:\n",
    "\t\t\ttest_traj_data_loader = DataLoader(test_traj_dataset,\n",
    "\t\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\tdrop_last=True)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\ttest_data_iter = iter(test_traj_data_loader)\n",
    "\t\t\ttimesteps, states, next_states, actions, rewards, traj_mask = next(test_data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train preprocessing(normalization, fit padding) data\n",
    "\n",
    "traj_dataset = D4RLTrajectoryDataset(DATA_PATH, k)\n",
    "traj_data_loader = DataLoader(traj_dataset,\n",
    "\t\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\tdrop_last=True)\n",
    "                        \n",
    "data_iter = iter(traj_data_loader)\n",
    "\n",
    "## get state stats from dataset\n",
    "state_mean, state_std = traj_dataset.get_state_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load validate preprocessing(normalization, fit padding) data\n",
    "\n",
    "val_traj_dataset = D4RLTrajectoryDataset(DATA_PATH, k, val=True, val_dataset_path=VAL_DATA_PATH)\n",
    "# val_traj_data_loader = DataLoader(val_traj_dataset,\n",
    "# \t\t\t\t\t\tbatch_size=batch_size,\n",
    "# \t\t\t\t\t\tshuffle=True,\n",
    "# \t\t\t\t\t\tpin_memory=True,\n",
    "# \t\t\t\t\t\tdrop_last=True)\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "model = DecisionTransformer(\n",
    "\t\t\tstate_dim=state_dim,\n",
    "\t\t\tact_dim=act_dim,\n",
    "\t\t\tn_blocks=n_blocks,\n",
    "\t\t\th_dim=embed_dim,\n",
    "\t\t\tcontext_len=k,\n",
    "\t\t\tn_heads=n_heads,\n",
    "\t\t\tdrop_p=drop_out,\n",
    "\t\t).to(DEVICE)\n",
    "  \n",
    "optimizer = torch.optim.AdamW(\n",
    "\t\t\t\t\tmodel.parameters(), \n",
    "\t\t\t\t\tlr=lr, \n",
    "\t\t\t\t\tweight_decay=wt_decay\n",
    "\t\t\t\t)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "\t\toptimizer,\n",
    "\t\tlambda steps: min((steps+1)/warmup_steps, 1)\n",
    "\t)\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now().replace(microsecond=0)\n",
    "\n",
    "start_time_str = start_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "prefix = \"dt_\" + env_name\n",
    "\n",
    "save_model_name =  prefix + \"_model_\" + str(max_train_iters) + \"_\" + str(batch_size) + \".pt\"\n",
    "save_model_path = os.path.join(LOG_PATH, save_model_name)\n",
    "save_best_model_path = save_model_path[:-3] + \"_best.pt\"\n",
    "\n",
    "log_csv_name = prefix + \"_log_\" + start_time_str + \".csv\"\n",
    "log_csv_path = os.path.join(LOG_PATH, log_csv_name)\n",
    "\n",
    "\n",
    "csv_writer = csv.writer(open(log_csv_path, 'a', 1))\n",
    "csv_header = ([\"duration\", \"num_updates\", \"total_loss\", \"state_loss\", \"reward_loss\", \"val_total_loss\", \"val_state_loss\", \"val_reward_loss\"])\n",
    "\n",
    "csv_writer.writerow(csv_header)\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"start time: \" + start_time_str)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"device set to: \" + str(DEVICE))\n",
    "print(\"dataset path: \" + DATA_PATH)\n",
    "print(\"model save path: \" + save_model_path)\n",
    "print(\"log csv save path: \" + log_csv_path)\n",
    "\n",
    "# train\n",
    "for i_train_iter in tqdm(range(max_train_iters)):\n",
    "\n",
    "\n",
    "\tlog_state_losses, log_reward_losses, log_total_losses = [], [], []\n",
    "\tval_log_state_losses, val_log_reward_losses, val_log_total_losses = [], [], []\n",
    "\tmodel.train()\n",
    "\t\n",
    "\tfor _ in range(num_updates_per_iter):\n",
    "\t\ttry:\n",
    "\t\t\ttimesteps, states, next_states, actions, rewards, traj_mask = next(data_iter)\n",
    "\t\texcept StopIteration:\n",
    "\t\t\ttraj_data_loader = DataLoader(traj_dataset,\n",
    "\t\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\tdrop_last=True)\n",
    "\t\t\tdata_iter = iter(traj_data_loader)\n",
    "\t\t\ttimesteps, states, next_states, actions, rewards, traj_mask = next(data_iter)\n",
    "\n",
    "\t\ttimesteps = timesteps.to(DEVICE)\t# B x T\n",
    "\t\tstates = states.to(DEVICE)\t\t\t# B x T x state_dim\n",
    "\t\tnext_states = next_states.to(DEVICE) # B X T X state_dim\n",
    "\t\tactions = actions.to(DEVICE)\t\t# B x T x act_dim\n",
    "\t\trewards = rewards.to(DEVICE).unsqueeze(dim=-1) # B x T x 1\n",
    "\t\ttraj_mask = traj_mask.to(DEVICE)\t# B x T\n",
    "\n",
    "\t\tnext_states_target = torch.clone(next_states).detach().to(DEVICE)\n",
    "\t\trewards_target = torch.clone(rewards).detach().to(DEVICE)\n",
    "\t\n",
    "\t\tnext_state_preds, rewards_preds = model.forward(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\ttimesteps=timesteps,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tstates=states,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tactions=actions,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\trewards=rewards,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\t\t# only consider non padded elements\n",
    "\t\tnext_state_preds = next_state_preds.view(-1, state_dim)[traj_mask.view(-1,) > 0]\n",
    "\t\tnext_states_target = next_states_target.view(-1, state_dim)[traj_mask.view(-1,) > 0]\n",
    "\t\t\n",
    "\t\trewards_preds = rewards_preds.view(-1, 1)[traj_mask.view(-1,) > 0]\n",
    "\t\trewards_target = rewards_target.view(-1, 1)[traj_mask.view(-1,) > 0]\n",
    "\n",
    "\t\tstate_loss = F.mse_loss(next_state_preds, next_states_target, reduction='mean') * state_weight\n",
    "\t\treward_loss = F.mse_loss(rewards_preds, rewards_target, reduction='mean') * reward_weight\n",
    "\t\t\n",
    "\t\ttotal_loss = state_loss.add(reward_loss)\n",
    "\t\ttotal_loss = torch.mean(total_loss)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\ttotal_loss.backward()\n",
    "\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "\t\toptimizer.step()\n",
    "\t\tscheduler.step()\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t#save loss\n",
    "\t\tlog_state_losses.append(state_loss.detach().cpu().item())\n",
    "\t\tlog_reward_losses.append(reward_loss.detach().cpu().item())\n",
    "\t\t\n",
    "\t\tlog_total_losses.append(total_loss.detach().cpu().item())\n",
    "\t\t\n",
    "\t# validation\n",
    "\tmodel.eval()\n",
    "\tval_traj_data_loader = DataLoader(val_traj_dataset,\n",
    "\t\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\tdrop_last=True)\n",
    "\tfor val_timesteps, val_states, val_next_states, val_actions, val_rewards, val_traj_mask in val_traj_data_loader:\n",
    "\t\t\n",
    "\t\tval_timesteps = val_timesteps.to(DEVICE)\t# B x T\n",
    "\t\tval_states = val_states.to(DEVICE)\t\t\t# B x T x state_dim\n",
    "\t\tval_next_states = val_next_states.to(DEVICE) # B X T X state_dim\n",
    "\t\tval_actions = val_actions.to(DEVICE)\t\t# B x T x act_dim\n",
    "\t\tval_rewards = val_rewards.to(DEVICE).unsqueeze(dim=-1) # B x T x 1\n",
    "\t\tval_traj_mask = val_traj_mask.to(DEVICE)\t# B x T\n",
    "\t\t\t\t\n",
    "\t\tval_next_states_target = torch.clone(val_next_states).detach().to(DEVICE)\n",
    "\t\tval_rewards_target = torch.clone(val_rewards).detach().to(DEVICE)\n",
    "\t\t\n",
    "\t\tval_next_state_preds, val_rewards_preds = model.forward(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\ttimesteps=val_timesteps,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tstates=val_states,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tactions=val_actions,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\trewards=val_rewards,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t# only consider non padded elements\n",
    "\t\tval_next_state_preds = val_next_state_preds.view(-1, state_dim)[traj_mask.view(-1,) > 0]\n",
    "\t\tval_next_states_target = val_next_states_target.view(-1, state_dim)[traj_mask.view(-1,) > 0]\n",
    "\t\t\n",
    "\t\tval_rewards_preds = val_rewards_preds.view(-1, 1)[traj_mask.view(-1,) > 0]\n",
    "\t\tval_rewards_target = val_rewards_target.view(-1, 1)[traj_mask.view(-1,) > 0]\n",
    "\n",
    "\t\tval_state_loss = F.mse_loss(val_next_state_preds, val_next_states_target, reduction='mean') * state_weight\n",
    "\t\tval_reward_loss = F.mse_loss(val_rewards_preds, val_rewards_target, reduction='mean') * reward_weight\n",
    "\n",
    "\t\t# todo: try to use mae\n",
    "\t\t\n",
    "\t\tval_total_loss = val_state_loss.add(val_reward_loss)\n",
    "\t\tval_total_loss = torch.mean(val_total_loss)\n",
    "\t\t\n",
    "\t\t# save val loss\n",
    "\t\tval_log_state_losses.append(val_state_loss.detach().cpu().item())\n",
    "\t\tval_log_reward_losses.append(val_reward_loss.detach().cpu().item())\n",
    "\t\t\n",
    "\t\tval_log_total_losses.append(val_total_loss.detach().cpu().item())\n",
    "\t\n",
    "\tmean_total_log_loss = np.mean(log_total_losses)\n",
    "\tmean_state_log_loss = np.mean(log_state_losses)\n",
    "\tmean_reward_log_loss = np.mean(log_reward_losses)\n",
    "\t\n",
    "\tmean_val_total_log_loss = np.mean(val_log_total_losses)\n",
    "\tmean_val_state_log_loss = np.mean(val_log_state_losses)\n",
    "\tmean_val_reward_log_loss = np.mean(val_log_reward_losses)\n",
    "\n",
    "\ttime_elapsed = str(datetime.now().replace(microsecond=0) - start_time)\n",
    "\n",
    "\ttotal_updates += num_updates_per_iter\n",
    "\n",
    "\tlog_str = (\"=\" * 60 + '\\n' +\n",
    "\t\t\t\"time elapsed: \" + time_elapsed  + '\\n' +\n",
    "\t\t\t\"num of updates: \" + str(total_updates) + '\\n' +\n",
    "\t\t\t\"train total loss: \" + format(mean_total_log_loss, \".5f\") + '\\n' +\n",
    "\t\t\t\"train state loss: \" + format(mean_state_log_loss, \".5f\") + '\\n' +\n",
    "\t\t\t\"train reward loss: \" +  format(mean_reward_log_loss, \".5f\") + '\\n' +\n",
    "\t\t\t\"val total loss: \" + format(mean_val_total_log_loss, \".5f\") + '\\n' +\n",
    "\t\t\t\"val state loss: \" + format(mean_val_state_log_loss, \".5f\") + '\\n' +\n",
    "\t\t\t\"val reward loss: \" +  format(mean_val_reward_log_loss, \".5f\")\n",
    "\t\t\t)\n",
    "\n",
    "\tprint(log_str)\n",
    "\n",
    "\tlog_data = [time_elapsed, total_updates, mean_total_log_loss, mean_state_log_loss, mean_reward_log_loss, \\\n",
    "\t\t mean_val_total_log_loss, mean_val_state_log_loss, mean_val_reward_log_loss]\n",
    "\n",
    "\tcsv_writer.writerow(log_data)\n",
    "\t\n",
    "\t# save model\n",
    "\tif mean_val_total_log_loss <= min_total_log_loss:\n",
    "\t\tprint(\"saving min loss model at: \" + save_best_model_path)\n",
    "\t\ttorch.save(model.state_dict(), save_best_model_path)\n",
    "\t\tmin_total_log_loss = mean_val_total_log_loss\n",
    "\n",
    "\tprint(\"saving current model at: \" + save_model_path)\n",
    "\ttorch.save(model.state_dict(), save_model_path)\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"finished training!\")\n",
    "print(\"=\" * 60)\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "time_elapsed = str(end_time - start_time)\n",
    "end_time_str = end_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "print(\"started training at: \" + start_time_str)\n",
    "print(\"finished training at: \" + end_time_str)\n",
    "print(\"total training time: \" + time_elapsed)\n",
    "print(\"saved min loss model at: \" + save_best_model_path)\n",
    "print(\"saved last updated model at: \" + save_model_path)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('rl_diffusion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aaa54a55e816925fdb1964dae2307e16b21383867fc9aac098dfe4376e4c067b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
